Cell 1 — Importing Libraries

We import all the required Python libraries:

pandas → for handling and cleaning dataset
numpy → for mathematical calculations
matplotlib & seaborn → for plotting graphs
sklearn → for machine learning models (Linear & Random Forest) and evaluation metrics


Cell 2 — Load Dataset - 
The dataset is loaded using read_csv() function to analyze the structure, column names, and data types.


Cell 3 — Dataset Info and Summary - 
info() shows data types and missing values
describe() gives statistical details like mean, min, max, etc.


Cell 4 — Pre-processing the Dataset
We remove:
Missing values → using dropna()
Duplicate entries → using drop_duplicates()


Cell 5 — Identify Outliers
Outliers are the extreme values that differ greatly from the rest of the data.
They can affect model performance and accuracy.

In this step, a boxplot is used to visualize outliers, and the IQR method (Interquartile Range) is applied to remove them.
Formula used:
IQR = Q3 - Q1
Records lying outside [Q1 - 1.5×IQR, Q3 + 1.5×IQR] are treated as outliers and removed.



Cell 6 — Correlation
The correlation matrix shows how numerical variables are related to each other.
It helps identify which features have a strong positive or negative impact on the target (fare_amount).
numeric_only=True ensures only numeric columns are used.
Values close to +1 = strong positive relation, −1 = strong negative relation, 0 = no relation.



Cell 7 — Split Data
X contains all numeric input features (independent variables).
y is the target variable (fare_amount).
train_test_split() divides data into training (80%) and testing (20%) sets.
random_state=42 ensures reproducibility.


Cell 8 — Linear Regression
Linear Regression is used to predict continuous values (like fare amount).
The model is trained using fit() and predictions are made with predict().
Performance is measured using:
R² Score: how well the model fits (closer to 1 = better)
RMSE: average prediction error (lower = better)


Cell 9 — Random Forest Regression
Random Forest is an ensemble method using multiple decision trees.
Each tree gives a prediction, and the average is taken as the final output.
It usually gives higher accuracy than Linear Regression.
Here, n_estimators=10 keeps runtime short for practical demo.


Cell 10 — Model Comparison
Compares both models based on R² score.
Higher R² indicates a better model performance.

This helps decide which algorithm fits the dataset better.

# Algorithm
⭐ Linear Model (Linear Regression) — Simple Algorithm

Step 1: Collect the dataset with input features and target variable.
Step 2: Preprocess the data (remove missing values, convert types).
Step 3: Separate the dataset into features (X) and target (y).
Step 4: Split the dataset into training and testing sets.
Step 5: Fit the linear regression model on the training data.
Step 6: Use the trained model to predict values on the test set.
Step 7: Evaluate the model using R² and RMSE.

⭐ Tree-Based Model (Random Forest) — Simple Algorithm

Step 1: Collect the dataset and preprocess it.
Step 2: Split the data into input features (X) and target (y).
Step 3: Divide the data into training and testing sets.
Step 4: Create a Random Forest model by choosing number of trees.
Step 5: Train the Random Forest on the training data.
Step 6: Predict the output for the test set.
Step 7: Evaluate performance using R² and RMSE.

#theory

⭐ 1) Linear Model (Linear Regression) — Theory

A linear model is a supervised learning algorithm used for predicting a continuous output.
It assumes that the relationship between input features and the output is linear and can be expressed as a straight-line equation:


Working

The model finds the best-fitting line by minimizing the mean squared error (MSE) between actual and predicted values.
This is done using the Least Squares Method.

Advantages

Simple to understand and implement

Very fast

Works well when data has a linear relationship

Disadvantages

Cannot capture non-linear patterns

Sensitive to outliers

Assumes independence between features

Evaluation Metrics

R² Score

Mean Squared Error (MSE)

Root Mean Squared Error (RMSE)

⭐ 2) Tree-Based Model (Random Forest Regressor) — Theory

A tree-based model uses decision trees to make predictions.
A decision tree splits the data based on conditions (rules) to reach a prediction.

A Random Forest is an ensemble method that builds multiple decision trees and outputs the average prediction of all trees.

Working

Multiple bootstrap samples are created from the training data.

A decision tree is trained on each sample.

At prediction time, all trees produce outputs.

The final prediction is the mean of these outputs.

Advantages

Handles non-linear relationships

Robust to noise and outliers

Reduces overfitting due to averaging

Works well on complex datasets

Disadvantages

Slower than linear models

Hard to interpret

Requires more memory

Evaluation Metrics

R² Score

RMSE

MAE (optional)

